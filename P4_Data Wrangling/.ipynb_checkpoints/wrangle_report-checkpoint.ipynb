{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did the Data Wrangling Project as the following order: Gathering, Assessing, Cleaning. <br>\n",
    "First, I did the Gathering. I used 'requests' library , downloaded the contents of image prediction csv, and saved it in the local directory as file named 'imagePredict.tsv'. <br>\n",
    "Since 'Twitter-Archive-enhanced.csv' was given for free, I saved it and loaded with 'pd.read_csv' function. <br>\n",
    "Lastly, since I got rejected to receive tweeter-development account, I manually obtained the 'tweet_json.txt' and loaded with Pandas DataFrame. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Assessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I opened up the 'Twitter-Archive-enhanced.csv' through Excel and had a quick look on it. I added Filter option and also sorted them according to some criteria such as alphabetical order. I spotted that some errors like <br>\n",
    "- 'rating_denominator' and 'rating_numerator' values are way above or below the average.\n",
    "- Some names were not correctly extracted. I spotted some names like 'a', 'not' etc.\n",
    "- Some column headers are values, not variable names (doggo, floofer pupper, puppo instead of 'stage')\n",
    "- Some unnecessary columns like 'in_reply_to_status_id', 'in_reply_to_user_id, 'retweeted_status_id', 'retweeted_status_user_id', 'expanded_urls'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, I used .info() function to look for any null values and data types. <br>\n",
    "I found out that 'timestamp' column can be converted to 'datetime64' instead of just 'obeject=string'. <br>\n",
    "Also, I thought 'in_reply_to_status_id' and 'retweeted_status_id' columns can be used to distinguish the original tweet from 'retweet' and 'replytweet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Tidininess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As instructed by Udacity, I firstly workd on correcting the Data Tidiness, aka stuructual issues. <br>\n",
    "I tried to approach this problem with using 'pd.melt' function, but then since some rows have multiple values, I could not find the decent and effective way to solve this issue. <br><br>\n",
    "Then I came up with another idea, which was to add the whole 4 types (doggo, floofer, pupper, and puppo) and manually replace the values. <br>\n",
    "- .replace('doggofloofer', 'doggo/floofer')\n",
    "- .replace('doggopupper', 'doggo/pupper')\n",
    "- .replace('doggopuppo', 'doggo/puppo')\n",
    "- .replace('flooferpupper', 'floofer/pupper')\n",
    "- .replace('pupperpoppo', 'pupper/puppo') <br>\n",
    "I stored this new string in the new colum names 'stage' and dropped the 4 columns (doggo, floofer, pupper, and puppo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I worked on merging the three separate tables. <br>\n",
    "Since all three tables have unique 'tweet_id' column, I was able to easily merge all three data by using <strong>'pd.merge(, how='left', on'id')'</strong> <br>\n",
    "sd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----Original Tweets----<br><br>\n",
    "I sorted out the rows that are original tweet by using the following method: <br>\n",
    "- cond1 = df_total['in_reply_to_status_id'].isnull()\n",
    "- cond2 = df_total['retweeted_status_id'].isnull()\n",
    "- df_total = df_total[cond1&cond2].copy() <br> <br>\n",
    "\n",
    "Then I dropped unnecessary columns and came up with ['id', 'timestamp', 'text', 'rating_numerator', 'rating_denominator', 'name','stage','retweet_count','favorite_count','img_num','p1','p1_conf'], which I would assumed to be essential for our project <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----rating_numerator----<br><br>\n",
    "Then I worked on tracking outlier in 'rating_numerator' values <br>\n",
    "After I had a brief look on the 'rating_numerator' values and 'value_counts()' function, I found out that there are also numerator values with decimal pts that were not correctly extract. <br>\n",
    "So, embracing Udacity's reviewer's feedback, I used the following code to correctly extract the 'rating_numerator' values form the 'text' <br>\n",
    "- df_total['text'].str.extract('((?:\\d+\\.)?\\d+)\\/(\\d+(\\.\\d+)?)', expand=True) <br><br>\n",
    "\n",
    "Then I changed the datatype to 'float' for numerator values and correctly raplce them with the newly extracted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----rating_denominator----<br><br>\n",
    "Same for 'rating_denominator' values, I spotted outliers with function '.value_counts()' and fixed the wrongly recorded denominator values by manually looking at the text. For the data that did not have any ratings, I replaced the value with 'np.nan' instead of 'None'. And I also changed its datatype to 'Int64' instead of 'int64' because 'np.nan' was regarded as float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----Fix the Wrong data in 'name' columne - Strange names like ['a', 'None', 'Not', 'actually', 'all']---- <br><br>\n",
    "\n",
    "I used .value_counts() function to look for wrongly recorded names. And I found unreasonable names like 'a', 'None', 'Not', 'actually', 'all'. By manually looking at the text, it seemed like some tweet do not contain the names at all and some tweets have different starting sentences which caused the wrong extraction of the name. I found a pattern that WeRateDogs sometimes puts the name after '~ ~ ~named {dog name}'. So I did some python string manipulation to extract the names for these irregular cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
